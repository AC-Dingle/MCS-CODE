import numpy as np
import os
import random
import matplotlib.pyplot as plt

# =======================
#  RFID CARDS (states)
# =======================
all_cards = [
    "06 0E 7F 40", "1D EC 43 14", "36 08 07 4E",
    "56 1D 12 4E", "96 34 81 8F", "96 E8 1B 4E",
    "83 4D 82 A9", "D6 61 89 5F", "96 DB 58 5F",
    "56 EA 94 5F", "D6 54 7B 5F", "D3 7B 36 11",
    "86 07 18 4E", "53 85 2F 22", "B3 38 3C 39"
]

num_states = len(all_cards)
num_actions = 2  # 0 = Unauthorized, 1 = Authorized

# =======================
#  Q-learning parameters
# =======================
alpha = 0.1      # Learning rate
gamma = 0.9      # Discount factor
episodes = 1000000  # Training episodes per run

# =======================
#  Load/Initialize Q-table and epsilon
# =======================
if os.path.exists("q_table.npy"):
    q_table = np.load("q_table.npy")
    print("‚úÖ Loaded existing Q-table.")
else:
    q_table = np.zeros((num_states, num_actions))
    print("‚ö†Ô∏è No saved Q-table found. Starting fresh.")

if os.path.exists("epsilon.npy"):
    epsilon = float(np.load("epsilon.npy"))
    print(f"‚úÖ Loaded epsilon: {epsilon:.4f}")
else:
    epsilon = 0
    print("‚ö†Ô∏è No saved epsilon found. Starting fresh (epsilon = 1.0).")

# =======================
#  Rewards
# =======================
reward_correct = 5
reward_wrong_auth = -10   # Big penalty (unauthorized accepted)
reward_wrong_unauth = -5  # Smaller penalty (authorized rejected)

# Load rewards history
if os.path.exists("rewards_history.npy"):
    rewards_history = np.load("rewards_history.npy").tolist()
else:
    rewards_history = []

episode_rewards = []

# =======================
#  Fixed Authorized/Unauthorized sets
# =======================
authorized_cards = {
    "06 0E 7F 40", "1D EC 43 14", "36 08 07 4E",
    "56 1D 12 4E", "96 34 81 8F", "96 E8 1B 4E", "83 4D 82 A9"
}
unauthorized_cards = set(all_cards) - authorized_cards

# =======================
#  Training loop with per-episode card log
# =======================
exploit_threshold = 0.2   # when epsilon < 0.2, agent mostly exploits
exploit_episode = None    # mark first exploit episode

for ep in range(episodes):
    # Gradually decay exploration rate
    epsilon = max(0.01, epsilon * 0.999998)

    total_reward = 0
    episode_log = []   # collect lines for this episode

    episode_log.append(f"\nEpisode {ep+1}/{episodes}")
    episode_log.append("Authorized cards:   " + ", ".join(sorted(authorized_cards)))
    episode_log.append("Unauthorized cards: " + ", ".join(sorted(unauthorized_cards)) + "\n")

    for state, card in enumerate(all_cards):
        # Œµ-greedy policy
        if random.uniform(0, 1) < epsilon:
            action = random.randint(0, num_actions - 1)
        else:
            action = np.argmax(q_table[state])

        # Reward logic and result
        if card in authorized_cards:
            if action == 1:
                reward = reward_correct
                result = "‚úÖ Correctly Authorized"
            else:
                reward = reward_wrong_unauth
                result = "‚ùå Mistakenly Unauthorized"
        else:
            if action == 0:
                reward = reward_correct
                result = "‚úÖ Correctly Blocked"
            else:
                reward = reward_wrong_auth
                result = "‚ùå WRONG! Unauthorized Accepted"

        # Q-learning update
        old_value = q_table[state, action]
        next_max = np.max(q_table[state])
        q_table[state, action] = old_value + alpha * (reward + gamma * next_max - old_value)

        total_reward += reward

        action_str = "Authorized" if action == 1 else "Unauthorized"
        episode_log.append(f"Card: {card} | Action: {action_str} | Result: {result}")

    episode_rewards.append(total_reward)

    # Add episode summary
    exploit_msg = ""
    if epsilon < exploit_threshold and exploit_episode is None:
        exploit_msg = "Agent is mostly exploiting now!"
        exploit_episode = len(rewards_history) + ep + 1
    episode_log.append(f"\nEpisode {ep+1} Total Reward: {total_reward} | "
                       f"Epsilon: {epsilon:.4f} {exploit_msg}")

    # ---- print the whole episode log at once ----
    print("\n".join(episode_log))

# =======================
#  Save Q-table, epsilon, and rewards history
# =======================
np.save("q_table.npy", q_table)
np.save("epsilon.npy", np.array(epsilon))
rewards_history.extend(episode_rewards)
np.save("rewards_history.npy", np.array(rewards_history))
print("\nüíæ Q-table, epsilon, and rewards history saved!")

# =======================
#  Running average helper
# =======================
def running_average(data, window=1000):
    if len(data) < window:
        return np.convolve(data, np.ones(len(data)) / len(data), mode="valid")
    return np.convolve(data, np.ones(window) / window, mode="valid")

# =======================
#  Show progress
# =======================
print(f"\nüèÜ Total reward this session: {sum(episode_rewards)}")
print(f"üìà Total episodes so far (all runs): {len(rewards_history)}")

plt.plot(rewards_history, label="Episode Reward")
if len(rewards_history) >= 1000:
    plt.plot(range(1000, len(rewards_history)+1),
             running_average(rewards_history, 1000),
             label="Running Avg (1000)", linewidth=2)

if exploit_episode is not None:
    plt.axvline(x=exploit_episode, color='red', linestyle='--', label="Mostly Exploiting")
    plt.text(exploit_episode + 2,
             max(rewards_history)*0.9 if rewards_history else 0,
             "Exploiting", color='red')

plt.xlabel("Episode")
plt.ylabel("Reward")
plt.title("Q-learning Training Progress")
plt.legend()
plt.show()
